
"""Fine-Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wd5oZXZh-jzve7GcAJ0ZuyQxdJ84KQ-F
"""

from Helper import Helper

h = Helper("https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip")

class_names = h.get_classnames()

class_names

h.view_random_test_image()

h.view_random_train_image()

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense , Conv2D, MaxPool2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
import datetime

# Importing the Data

BATCH_SIZE = 32
IMG_SIZE = (224,224)

train_data = tf.keras.utils.image_dataset_from_directory(
    directory=train_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True,
    label_mode='categorical'

)

test_data = tf.keras.utils.image_dataset_from_directory(
    directory=test_dir,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical',
    shuffle=False
)



print(train_data.class_names,test_data.class_names)


def create_callbacks(dir_name, exp_name):
  log_dir = dir_name + "/" + exp_name + "_" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensor_callbacks = tf.keras.callbacks.TensorBoard(log_dir="./logs" )
  print(f"Dumping into the dir {log_dir}")
  return tensor_callbacks

"""
## Setting Up a Model
"""

tf.random.set_seed(42)

base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)

# Freezing all the layers except last 5 layers 

for layer_no , layer_name in enumerate(base_model.layers[:-5]):
  if layer_name.trainable == True : layer_name.trainable =  False

# Printing the layers 

for layer_no , layer_name in enumerate(base_model.layers[:]):
  if layer_name.trainable == True:
    print(layer_no ,layer_name.name , layer_name.trainable)

# Creating Input Layer 

inputs = tf.keras.layers.Input(shape=(224,224,3), name="Input_Layer")

print("Shape of inputs  : ", inputs.shape)

## Data Augmentation Layer

data_augmentaion_layer = Sequential(
    [
  layers.RandomFlip(mode='horizontal'),
  layers.RandomZoom(0.2),
  layers.RandomHeight(0.2),
  layers.RandomWidth(0.2),
  layers.RandomRotation(0.2)
  ]
)

# Augmenting the data 
X = data_augmentaion_layer(inputs)

print("Shape after aug layer  : ", X.shape)

# Adding the base model 
X = base_model(X, training=False)

print("Shape after passing through base model  : ", X.shape)

# Global Average Pooling Layer 

X = tf.keras.layers.GlobalAveragePooling2D(name="Global_Avg_Pool_Layer") ( X )

print("Shape after passing through Avg Pooling Layer  : ", X.shape)

# Output layer 

outputs = tf.keras.layers.Dense(
    len(class_names),
    activation='softmax',
    name='Output_Layer'
) (X)

print("Shape after output layer : ", outputs.shape)

# Fined Tuned Model 

fine_tuned_model = tf.keras.Model(inputs , outputs)

# Printing the layers 

for layer_no , layer_name in enumerate(fine_tuned_model.layers):
  print(layer_no , layer_name.name , layer_name.trainable)
  if layer_name.name == "efficientnetv2-b0":
    for no , name in enumerate(layer_name.layers[-10:]):
      print('\t', 270 - 10 + no , name.name, name.trainable)

"""## Model Compilation"""

## Compilation of fine-tunned model

fine_tuned_model.compile(
    loss=tf.keras.losses.categorical_crossentropy,
    optimizer = tf.keras.optimizers.Adam(0.0001),
    metrics=['accuracy']
)


## Model Checkpoint 

create_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath="10_percent_data_checkpoint/checkpoint.ckpt",
                                                             save_weights_only=True,
                                                             save_freq="epoch",
                                                             save_best_only = True,
                                                             verbose=1)

## Model Fitting 

history1 = fine_tuned_model.fit(train_data,
                     epochs=5,
                     steps_per_epoch=len(train_data),
                     batch_size=BATCH_SIZE,
                     validataion_data = test_data ,
                     validation_steps = int(0.25 * len(test_data)),
                     callbacks=[create_callbacks("EfficientNets", "10_per_data"), create_model_checkpoint]
                     )

print(len(train_data) ,len(class_names))

